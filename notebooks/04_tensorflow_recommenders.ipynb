{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Multitask Recommender\n",
    "\n",
    "Tensorflow recommenders is a library for building recommender system models.\n",
    "\n",
    "It does this by performing two tasks:\n",
    "- Retrieval: Finding candidates that are likely to be relevant to the user given history or context features\n",
    "- Ranking: Sorting retrieved items in order of relevance to the user using an assign score or explicit feedback.\n",
    "\n",
    "For more information, please go through [TensorFLow Recommenders: Quickstart](https://www.tensorflow.org/recommenders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_recommenders as tfrs\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "df = pd.read_csv(\"../data/processed/clean_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the percentage of unique values we have in each feature\n",
    "for col in df.columns:\n",
    "    print(f\"{col:} has {df[col].nunique():,} unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the df to dictionary\n",
    "df_dict = {name: np.array(val) for name, val in df.items()}\n",
    "\n",
    "# converting df dictionary to tensor slices\n",
    "data = tf.data.Dataset.from_tensor_slices(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of unique values \n",
    "vocabularies = {}\n",
    "\n",
    "for feature in df_dict:\n",
    "    if feature != 'Feedback':\n",
    "        vocab = np.unique(df_dict[feature])\n",
    "        vocabularies[feature] = vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting unique anime names to tensorflow dataset\n",
    "anime_names = tf.data.Dataset.from_tensor_slices(vocabularies['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffling and splitting the dataset\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "shuffled = data.shuffle(500_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(240_000)\n",
    "validation = shuffled.skip(240_000).take(55_000)\n",
    "test = shuffled.skip(295_000).take(10_358)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model building\n",
    "\n",
    "Three classes will be constructed to build a multitask hybrid recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# responsible for building the user model\n",
    "\n",
    "class UserModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        max_tokens = 10_000\n",
    "        \n",
    "        # Genre -> splits genres, creates vectors based on splits and then a 32 dimensional embedding\n",
    "        self.genre_vectorizer = keras.layers.TextVectorization(max_tokens=max_tokens, split=\"whitespace\")\n",
    "        self.genre_vectorizer.adapt(vocabularies['genre'])\n",
    "        self.genre_text_embedding = keras.Sequential([\n",
    "            self.genre_vectorizer,\n",
    "            keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
    "            keras.layers.GlobalAveragePooling1D()\n",
    "        ])\n",
    "        \n",
    "        \n",
    "        # Type -> 5 unique types, so StringLookup to map type to int value, and then 32 dimensional embedding\n",
    "        self.type_embedding = keras.Sequential([\n",
    "            keras.layers.StringLookup(\n",
    "                vocabulary=vocabularies['type'],\n",
    "                mask_token=None),\n",
    "            keras.layers.Embedding(len(vocabularies['type'])+1, 32)\n",
    "        ])\n",
    "        \n",
    "        \n",
    "        # Audience -> same as Type, but with 3 unique types\n",
    "        self.audience_embedding = keras.Sequential([\n",
    "            keras.layers.StringLookup(\n",
    "                vocabulary=vocabularies['Audience'],\n",
    "                mask_token=None),\n",
    "            keras.layers.Embedding(len(vocabularies['Audience'])+1, 32)\n",
    "        ])\n",
    "        \n",
    "        \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # concatenating all embeddings\n",
    "        return tf.concat([\n",
    "            self.genre_text_embedding(inputs['genre']),\n",
    "            self.type_embedding(inputs['type']),\n",
    "            self.audience_embedding(inputs['Audience'])\n",
    "        ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# responsible for building the anime model\n",
    "\n",
    "class AnimeModel(keras.Model):\n",
    "    \n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "        \n",
    "        max_tokens = 10_000\n",
    "        \n",
    "        # Anime name -> 990+ unique names so Vectorization will be applied w/out splits, then embedded\n",
    "        self.anime_vectorizer = keras.layers.TextVectorization(max_tokens=max_tokens)\n",
    "        self.anime_vectorizer.adapt(anime_names)\n",
    "        self.anime_text_embedding = keras.Sequential([\n",
    "            self.anime_vectorizer,\n",
    "            keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
    "            keras.layers.GlobalAveragePooling1D()\n",
    "        ])\n",
    "        \n",
    "        # stringlookup \n",
    "        self.anime_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=vocabularies['name'],\n",
    "                mask_token=None),\n",
    "            tf.keras.layers.Embedding(len(vocabularies['name'])+1, 32)\n",
    "        ])\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # concatenating all embeddings\n",
    "        return tf.concat([\n",
    "            self.anime_embedding(inputs),\n",
    "            self.anime_text_embedding(inputs),\n",
    "        ], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting random seed\n",
    "tf.random.set_seed(7)\n",
    "np.random.seed(7)\n",
    "\n",
    "# tensorflow recommenders model\n",
    "class TFRSModel(tfrs.models.Model):\n",
    "    \n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "        \n",
    "        # handles how much weight we want to assign to the rating and retrieval task when computing loss\n",
    "        self.rating_weight = 0.5\n",
    "        self.retrieval_weight = 0.5\n",
    "        \n",
    "        \n",
    "        # UserModel\n",
    "        self.user_model = keras.Sequential([\n",
    "            UserModel(),\n",
    "            keras.layers.Dense(32)\n",
    "        ])\n",
    "        \n",
    "        \n",
    "        # AnimeModel\n",
    "        self.anime_model = keras.Sequential([\n",
    "            AnimeModel(),\n",
    "            keras.layers.Dense(32)\n",
    "        ])\n",
    "        \n",
    "        \n",
    "        # Deep & Cross layer\n",
    "        self._cross_layer = tfrs.layers.dcn.Cross(projection_dim=None, kernel_initializer='he_normal')\n",
    "        \n",
    "        \n",
    "        # Dense layers with l2 regularization to prevent overfitting (basic layers - activation can be 'swish' with a he_normal kernel_initializer to improve performace - no need for experimenting here)\n",
    "        # keras.layers.Dense(512, activation='swish', kernel_initializer='he_normal')\n",
    "        self._deep_layers = [\n",
    "            keras.layers.Dense(512, activation='relu', kernel_regularizer='l2'),\n",
    "            keras.layers.Dense(256, activation='relu', kernel_regularizer='l2'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Dropout(0.2),\n",
    "            keras.layers.Dense(128, activation='relu', kernel_regularizer='l2'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Dropout(0.3),\n",
    "            keras.layers.Dense(64, activation='relu', kernel_regularizer='l2'),\n",
    "            keras.layers.Dense(32, activation='relu', kernel_regularizer='l2'),\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        # output layer\n",
    "        self._logit_layer = keras.layers.Dense(1)\n",
    "    \n",
    "        # Multi-task Retrieval & Ranking\n",
    "        self.rating_task: tf.keras.layers.Layer = tfrs.tasks.Ranking(\n",
    "            loss=tf.keras.losses.MeanSquaredError(),\n",
    "            metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
    "        )\n",
    "        self.retrieval_task: tf.keras.layers.Layer = tfrs.tasks.Retrieval(\n",
    "            metrics=tfrs.metrics.FactorizedTopK(\n",
    "                candidates=anime_names.batch(128).map(self.anime_model)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        \n",
    "    # calls user and anime embeddings, applies cross_layers, keras.layers, and output layer defined above -> returns embeddings and output layer\n",
    "    def call(self, features) -> tf.Tensor:\n",
    "        user_embeddings = self.user_model({\n",
    "            'genre': features['genre'],\n",
    "            'type': features[\"type\"],\n",
    "            'Audience': features[\"Audience\"]\n",
    "        })\n",
    "        \n",
    "        \n",
    "        anime_embeddings = self.anime_model(\n",
    "            features['name']\n",
    "        )\n",
    "        \n",
    "        x = self._cross_layer(tf.concat([\n",
    "                user_embeddings,\n",
    "                anime_embeddings], axis=1))\n",
    "        \n",
    "        for layer in self._deep_layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        \n",
    "        return (\n",
    "            user_embeddings, \n",
    "            anime_embeddings,\n",
    "            self._logit_layer(x)\n",
    "        )\n",
    "        \n",
    "    \n",
    "    # computes the ranking and retrieval loss using embeddings and dense layers\n",
    "    def compute_loss(self, features, training=False):\n",
    "        user_embeddings, anime_embeddings, rating_predictions = self.call(features)\n",
    "        # Retrieval loss\n",
    "        retrieval_loss = self.retrieval_task(user_embeddings, anime_embeddings)\n",
    "        # Rating loss\n",
    "        rating_loss = self.rating_task(\n",
    "            labels=features['Feedback'],\n",
    "            predictions=rating_predictions\n",
    "        )\n",
    "        \n",
    "        # Combine two losses with hyper-parameters (to be tuned)\n",
    "        return (self.rating_weight * rating_loss + self.retrieval_weight * retrieval_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batching and caching our datasets to improve performance\n",
    "\n",
    "cached_train = train.shuffle(400_000).batch(2000).cache()\n",
    "cached_validation = validation.shuffle(100_000).batch(2000).cache()\n",
    "cached_test = test.shuffle(50_000).batch(2000).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for building the model\n",
    "\n",
    "def create_model():\n",
    "    \"\"\" instantiates a model anmd compiles it \"\"\"\n",
    "    \n",
    "    keras.backend.clear_session()\n",
    "    tf.random.set_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # instantiating the model\n",
    "    model = TFRSModel()\n",
    "    model.compile(optimizer=keras.optimizers.Adam(0.01))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "# keras callbacks\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(monitor=\"val_factorized_top_k/top_10_categorical_accuracy\", patience=5) # reduces learning rate if 'monitor' doesn't improve after 'patience' steps \n",
    "ModelCheckpoint_cb = keras.callbacks.ModelCheckpoint(filepath=\"../model/checkpoints/TFRS_MultiTaskModel {epoch:02d}\", save_weights_only=True) # saves model weights after each epoch\n",
    "EarlyStopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_factorized_top_k/top_10_categorical_accuracy\", restore_best_weights=True, patience=10) # stops training if model performance doesn't improve\n",
    "\n",
    "log_dir = \"../model/logs/fit/\" + datetime.datetime.now().strftime(\"%Y_%m_%d-%H:%M:%S\")\n",
    "tensorboar_cb = keras.callbacks.TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the model \n",
    "history_fit = model.fit(cached_train, validation_data=cached_validation, callbacks=[ModelCheckpoint_cb, EarlyStopping_cb, lr_scheduler, tensorboar_cb], epochs=20, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model\n",
    "filepath = \"../model/model_weights/\"\n",
    "\n",
    "model.save_weights(filepath=filepath, save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the saved model weights\n",
    "\n",
    "filepath = \"../model/model_weights/\"\n",
    "\n",
    "# loading the model\n",
    "model = create_model()\n",
    "\n",
    "# load the weights back to the new model\n",
    "model.load_weights(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing training and validation logs in Tensorboard\n",
    "\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --log_dir=\"../model/logs/fit/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "an easier option is just plotting the training and validation learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning curves\n",
    "learning_curve_data = history_fit.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing model learning curves\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20,6))\n",
    "\n",
    "ax[0].plot(learning_curve_data['root_mean_squared_error'], label='training')\n",
    "ax[0].plot(learning_curve_data['val_root_mean_squared_error'], label='validation')\n",
    "ax[0].set_xlabel('steps', fontsize=12)\n",
    "ax[0].set_ylabel('root_mean_squared_error', fontsize=12)\n",
    "\n",
    "ax[1].plot(learning_curve_data['factorized_top_k/top_5_categorical_accuracy'], label='training')\n",
    "ax[1].plot(learning_curve_data['val_factorized_top_k/top_5_categorical_accuracy'], label='validation')\n",
    "ax[1].set_xlabel('steps', fontsize=12)\n",
    "ax[1].set_ylabel('top_5_categorical_accuracy', fontsize=12)\n",
    "\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "\n",
    "fig.text(x=0.5, y=0.95, s=\"Model learning curves\", fontsize=15, weight='bold', ha='center', va='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating the model\n",
    "scores = model.evaluate(cached_test, return_dict=True, verbose=False)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`top_k_categorical_accuracy` represents the percentage of times the top-ranked recommendation matches the user's preference.\n",
    "\n",
    "i.e a `top_5_categorical_accuracy` of 0.9963 means that in 99.63% of the cases, the preferred / correct item appeard in the top 5 recommended items. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inference\n",
    "\n",
    "More Experimentation can be done to improve model performance, but a top_5 of 99.63% is great! for this purpose of this project, so it will be left as is.\n",
    "\n",
    "Check `scripts` and `app` on how to deploy the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "steam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
